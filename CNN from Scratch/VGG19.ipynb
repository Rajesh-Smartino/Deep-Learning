{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d492d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "badfd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from matplotlib import pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "602aecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \n",
    "    train_path = path+\"/train\"\n",
    "    test_path = path+\"/test\"\n",
    "    validation_path = path+\"/val\"\n",
    "    \n",
    "    tr_data, test_data, val_data = [], [], []\n",
    "    tr_out, test_out, val_out = [], [], []\n",
    "\n",
    "    for i in os.listdir(train_path):\n",
    "        try:\n",
    "            for j in os.listdir(train_path+\"/\"+i):\n",
    "                image = cv2.imread(train_path+\"/\"+i+\"/\"+j) #COLOR_BGR2RGB\n",
    "                tr_data.append(cv2.resize(image, (224, 224)))\n",
    "                tr_out.append(i)\n",
    "\n",
    "            for j in os.listdir(test_path+\"/\"+i):\n",
    "                image = cv2.imread(test_path+\"/\"+i+\"/\"+j)\n",
    "                test_data.append(cv2.resize(image, (224, 224)))\n",
    "                test_out.append(i)\n",
    "\n",
    "            for j in os.listdir(validation_path+\"/\"+i):\n",
    "                image = cv2.imread(validation_path+\"/\"+i+\"/\"+j)\n",
    "                val_data.append(cv2.resize(image, (224, 224)))\n",
    "                val_out.append(i)\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    tr_data, test_data, val_data = np.array(tr_data), np.array(test_data), np.array(val_data)\n",
    "    return tr_data, test_data, val_data, tr_out, test_out, val_out\n",
    "\n",
    "def label_data(x):\n",
    "    label = []\n",
    "    for i in x:\n",
    "        if i =='chandelier':\n",
    "            label.append(0)\n",
    "        if i =='ketch':\n",
    "            label.append(1)\n",
    "        if i =='bonsai':\n",
    "            label.append(2)\n",
    "    return label\n",
    "\n",
    "path = \"/Users/rajeshr/Desktop/Group_22\"\n",
    "tr_data, test_data, val_data, tr_out, test_out, val_out = read_data(path)\n",
    "\n",
    "test_label = np.array(label_data(test_out))\n",
    "tr_label = np.array(label_data(tr_out))\n",
    "val_label = np.array(label_data(val_out))\n",
    "\n",
    "#### SELECTED IMAGES \n",
    "path = '/Users/rajeshr/Desktop/Group_22/selected/'\n",
    "images = []\n",
    "color_images = []\n",
    "label = []\n",
    "for i in os.listdir(path):\n",
    "    image = cv2.imread(path+i, cv2.IMREAD_GRAYSCALE)\n",
    "    color_image = cv2.imread(path+i, cv2.COLOR_BGR2RGB)\n",
    "    try:\n",
    "        images.append(cv2.resize(image, (224, 224)))\n",
    "        color_images.append(cv2.resize(color_image, (224, 224)))\n",
    "        label.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "images = np.array(images)\n",
    "label = np.array(label)\n",
    "color_images = np.array(color_images)\n",
    "labels = np.arange(0, len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2cacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE  = 224\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714417b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vgg model\n",
    "# load the model\n",
    "model = VGG19()\n",
    "# summarize the model\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14eb78ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 (3, 3, 3, 64)\n",
      "block1_conv2 (3, 3, 64, 64)\n",
      "block2_conv1 (3, 3, 64, 128)\n",
      "block2_conv2 (3, 3, 128, 128)\n",
      "block3_conv1 (3, 3, 128, 256)\n",
      "block3_conv2 (3, 3, 256, 256)\n",
      "block3_conv3 (3, 3, 256, 256)\n",
      "block3_conv4 (3, 3, 256, 256)\n",
      "block4_conv1 (3, 3, 256, 512)\n",
      "block4_conv2 (3, 3, 512, 512)\n",
      "block4_conv3 (3, 3, 512, 512)\n",
      "block4_conv4 (3, 3, 512, 512)\n",
      "block5_conv1 (3, 3, 512, 512)\n",
      "block5_conv2 (3, 3, 512, 512)\n",
      "block5_conv3 (3, 3, 512, 512)\n",
      "block5_conv4 (3, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# summarize filters in each convolutional layer\n",
    "# load the model\n",
    "# summarize filter shapes\n",
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4baddb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAB0CAYAAAC7Ueh1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADMUlEQVR4nO3YoU48ZxTG4TPNwAqyGwSDrSDBYtZDCPeC5w6waAwKyW2gCALHDfA3CEowBIHhq6gsTXbI0Je0zyM3k3O+7Mz8stmutVYA/Pt+Sx8A4P9KgAFCBBggRIABQgQYIESAAUL6MRevra212Wz2XWdZydvbW3T/T9Fa66aatVgs2jAMU437kr4f9Sh+i/l8Ht3/8PBQz8/Pk91X7+tfum6yr/TLWmvPrbW/vWSjnvrZbFZ7e3vTneoLbm9vo/urqj4+PtJHmNQwDHV2dhY9w9bWVnR/VdX+/n50/3K5nHTeT3hfb25uovurqtbX19NHqPf391+ffe4vCIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBgjpx1y8WCzq8PDwu86yksvLy+j+qqrd3d3o/uVyOem8p6enOj8/n3TmWNfX19H9VVXb29vR/S8vL5POG4ahjo+PJ5051unpaXR/VdX9/X36CHVycvLp534BA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMEBIP+bi+XxeBwcH33SU1dzd3UX3V1VdXV1F9z8+Pk46r+/72tzcnHTmWDs7O9H9VVUbGxvR/a+vr5PO67qu+n7UKz65o6Oj6P6qqouLi/QR/pFfwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQ0rXWVr+46/6oql/fdxxW9HtrbZhqmPv6Y7iv/12f3ttRAQZgOv6CAAgRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYI+RPMs2S7x3QFHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "# plot first few filters\n",
    "n_filters, ix = 1, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9490db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = tf.keras.applications.VGG19(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n",
    "#pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10f07a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "    #print(layer.name)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd3714fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = pre_trained_model.get_layer('block5_pool')\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9539715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.GlobalMaxPooling2D()(last_output)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bf34b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(pre_trained_model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73d31527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd346c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 3 classes.\n",
      "Found 30 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale = 1./255)\n",
    "train_data_gen = image_gen_train.flow_from_directory(batch_size = batch_size, \n",
    "                                                     directory = '/Users/rajeshr/Desktop/Group_22/train',\n",
    "                                                     shuffle= True,\n",
    "                                                     target_size = (IMG_SHAPE,IMG_SHAPE),\n",
    "                                                     class_mode = 'categorical')\n",
    "\n",
    "image_generator_validation = ImageDataGenerator(rescale=1./255)\n",
    "val_data_gen = image_generator_validation.flow_from_directory(batch_size=batch_size,\n",
    "                                                              directory='/Users/rajeshr/Desktop/Group_22/val',\n",
    "                                                              target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                                              class_mode='categorical')\n",
    "image_gen_test = ImageDataGenerator(rescale=1./255)\n",
    "test_data_gen = image_gen_test.flow_from_directory(batch_size=batch_size,\n",
    "                                                   directory='/Users/rajeshr/Desktop/Group_22/test',\n",
    "                                                   target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                                   class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b51692bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 2/25\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 3/25\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 4/25\n",
      "4/4 [==============================] - 18s 5s/step - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 5/25\n",
      "4/4 [==============================] - 18s 4s/step - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 6/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 7/25\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 8/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 9/25\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 10/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 11/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 12/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 13/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 14/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 15/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 16/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 17/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 18/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 19/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 20/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 21/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 22/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 23/25\n",
      "4/4 [==============================] - 19s 5s/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 24/25\n",
      "4/4 [==============================] - 21s 5s/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 25/25\n",
      "4/4 [==============================] - 20s 5s/step - loss: 0.0040 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "vgg_classifier = model.fit(train_data_gen,\n",
    "                 steps_per_epoch=(len(tr_data)//batch_size),\n",
    "                 epochs = 25,\n",
    "                 validation_data=val_data_gen,\n",
    "                 validation_steps=(len(val_data)//batch_size),\n",
    "                 batch_size = batch_size,\n",
    "                 verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb8e2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 8s 4s/step - loss: 0.0220 - acc: 1.0000\n",
      "test_loss, test accuracy [0.02201954834163189, 1.0]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data_gen,batch_size=batch_size)\n",
    "print(\"test_loss, test accuracy\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fde9a218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/Users/rajeshr/JupyterNotebooks/DeepLearning/A3/VGG_Caltech_Classifier.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"/Users/rajeshr/JupyterNotebooks/DeepLearning/A3/VGG_Caltech_Classifier.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model.save_weights(\"/Users/rajeshr/JupyterNotebooks/DeepLearning/A3/VGG_Caltech.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99019d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efa76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
