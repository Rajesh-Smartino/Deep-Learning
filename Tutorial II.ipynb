{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35216448",
   "metadata": {},
   "source": [
    "# CS671 Deep Learning & Applications - Tutorial II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e22fbe",
   "metadata": {},
   "source": [
    "Date: 18 April 2023 | Instructor: Dr. Dileep A.D. | References: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05b8c1",
   "metadata": {},
   "source": [
    "### CNN Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64500908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dropout, Dense, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model, save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff06deb",
   "metadata": {},
   "source": [
    "##### Simple 10 Class Image Classification N/w using Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c57cad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((224, 224, 1))\n",
    "\n",
    "### LAYER 1\n",
    "l1 = Conv2D(32, (3, 3), activation='relu')(inp)\n",
    "l1 = MaxPooling2D((3, 3))(l1)\n",
    "\n",
    "### LAYER 2\n",
    "l2 = Conv2D(64, (3, 3), activation='relu')(l1)\n",
    "l2 = MaxPooling2D((3, 3))(l2)\n",
    "\n",
    "### LAYER 3\n",
    "l3 = Conv2D(128, (3, 3), activation='relu')(l2)\n",
    "l3 = MaxPooling2D((3, 3))(l3)\n",
    "\n",
    "### Flattening\n",
    "flatten = Flatten()(l3)\n",
    "\n",
    "### Fully Connected Layers\n",
    "\n",
    "l4 = Dense(256, activation='relu')(l3)\n",
    "output = Dense(10, activation='softmax')(l4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec0f241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inp, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171078bb",
   "metadata": {},
   "source": [
    "### Pre-trained Models: VGG16/ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141366b",
   "metadata": {},
   "source": [
    "Whole list:  https://keras.io/api/applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c913e56",
   "metadata": {},
   "source": [
    "#### Direct Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2737bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84198d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ead0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(preds, top=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec8c3e",
   "metadata": {},
   "source": [
    "#### Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c265618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca59a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8980e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3dd9a",
   "metadata": {},
   "source": [
    "#### Extracting Intermediate Layer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b2817",
   "metadata": {},
   "source": [
    "#### Finetuning for New Sets of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb46ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eed9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e48246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(...)\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883d01f",
   "metadata": {},
   "source": [
    "#### Custom Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "model = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b0a1a",
   "metadata": {},
   "source": [
    "### Visualizing CNN and Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        filters, biases = layer.get_weights()\n",
    "        print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37f173",
   "metadata": {},
   "source": [
    "##### Visualize kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "\n",
    "filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "f_min, f_max = filters.min(), filters.max() # Normalising\n",
    "filters = (filters - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a03a8",
   "metadata": {},
   "source": [
    "##### Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = model.predict(img)\n",
    "\n",
    "ix = 2 # ith number of feature in corresponding layer.\n",
    "plt.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b6f71",
   "metadata": {},
   "source": [
    "### Visualizing Patches that Maximally Activate a Neuron, Visualizing Influence of Input Pixel (Guided Back Propagation/ GradCAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define some input data and trainable variables\n",
    "x = tf.constant(3.0)\n",
    "w = tf.Variable(4.0)\n",
    "\n",
    "# Define a function to compute y = w*x\n",
    "def compute_y(x, w):\n",
    "    return w*x\n",
    "\n",
    "# Use GradientTape to compute gradients of y with respect to w\n",
    "with tf.GradientTape() as tape:\n",
    "    y = compute_y(x, w)\n",
    "\n",
    "grads = tape.gradient(y, w)\n",
    "\n",
    "# Print the gradients\n",
    "print(grads.numpy()) # Output: 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c11e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
